# -*- coding: utf-8 -*-
"""機器學習_白酒分類跟預測 hw1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uB6tv38nfPCHZJAh6FtXONsQn-ZsUUf3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

whitequality_data = pd.read_csv("/content/drive/MyDrive/winequality-white.csv")

whitequality_data

"""# 刪除空白植跟不必要欄位

"""

whitequality_data.isnull().any()#無缺失值

whitequality_data.describe()

"""# 觀察特徵值"""

from sklearn.ensemble import RandomForestRegressor
Y=whitequality_data["quality"]
df=whitequality_data.drop(['quality'], axis=1)

model_Forest = RandomForestRegressor(random_state=1, max_depth=10)
model_Forest.fit(df,Y)

features = df.columns
importances = model_Forest.feature_importances_
indices = np.argsort(importances[0:10])  # top 10 features
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""#資料平衡"""

whitequality_data['quality'].value_counts()

from imblearn.over_sampling import RandomOverSampler  
X = whitequality_data.iloc[:,0:11].values 
y= whitequality_data['quality'].values  
ros = RandomOverSampler()  
X,y = ros.fit_sample(X,y)  
print(pd.DataFrame(y)[0].value_counts().sort_index())

"""# 標準化"""

#標準化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X)
X_scaler = scaler.transform(X)

#Y值修改
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y1 = encoder.fit_transform(y)
Y = pd.get_dummies(y1).values
Y[:]

"""# 分割資料集"""

#預測數值資料集
res_X_train, res_X_test, res_y_train, res_y_test = train_test_split(X_scaler,y,test_size=0.2,random_state=0)

#分類數值資料集
X_train, X_test, y_train, y_test = train_test_split(X_scaler,Y,test_size=0.2,random_state=0)

"""# 預測數值

"""

from keras import models
from keras import layers
from keras.optimizers import RMSprop

#build_model
#調整不同的lr
model = models.Sequential()
model.add(layers.Dense(64,activation='relu',input_shape=(11,)))
model.add(layers.Dense(128,activation='relu'))
model.add(layers.Dense(1))
rmsprop = RMSprop(lr=0.002)
model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])
model.fit(res_X_train,res_y_train,epochs=80,batch_size=16,verbose=0)
test_mse,test_mae = model.evaluate(res_X_test,res_y_test)

res_y_pred = model.predict(res_X_test)

from sklearn.metrics import mean_squared_error
import math

#mape
def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
mape = mean_absolute_percentage_error(res_y_test, res_y_pred)
print(mape)

#rmse
RMSE = math.sqrt(test_mse)
RMSE

#模型
model.summary()

"""# 分類"""

from keras.layers import Dropout

model = Sequential()
model.add(Dense(500, input_dim=11, activation='relu'))
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(50, activation='sigmoid'))
model.add(Dense(7, activation='softmax'))
rmsprop = RMSprop(lr=0.002)
model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train,validation_data  = (X_test,y_test), epochs=8, batch_size=16)

y_pred = model.predict(X_test)
y_test_class = np.argmax(y_test, axis=1)
y_pred_class = np.argmax(y_pred, axis=1)

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test_class, y_pred_class))
print(confusion_matrix(y_test_class, y_pred_class))

score = model.evaluate(X_test,y_test)
print(score)